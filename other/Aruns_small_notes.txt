========================================================================
### sqoop snappy compress
====================================

--compress \
--compression-codec org.apache.hadoop.io.compress.SnappyCodec

--compress \
--compression-codec snappy

========================================================================
### sqoop save as text
### sqoop save as avro
### sqoop save as parquet
====================================

--as-textfile
--as-avrodatafile
--as-parquetfile

========================================================================
### sqoop import: handle numbers as null and string null in sqoop import
====================================

--null-non-string '\\N' \
--null-string '\\N'

========================================================================
### sqoop export: set delimiter from hive to mysql export 
###	handle numbers as null and string null in sqoop export
====================================

--input-fields-terminated-by '\0001' \
--input-null-non-string '\\N' \
--input-null-string '\\N'


========================================================================
### sqoop export: myhivedb.db/myhtable using id,name desc fields
====================================

--export-dir /user/hive/warehouse/myhivedb.db/myhtable/ \
--columns "id,name desc"

========================================================================
### sqoop merge steps
====================================

sqoop merge --help
run a sqoop job for a jar file

========================================================================
### sqoop job : create my_test_job and import
====================================

sqoop job --create my_test_job \
-- import


========================================================================
### sqoop job : incremental import 3 main commands
====================================

--check-column product_id \
--incremental append \
--last-value 0



========================================================================
### sqoop : hive import to myhivedatabase and myhtable
====================================

--hive-import \
--hive-database myhivedatabase \
--hive-table myhtable


========================================================================
*
### sqoop : hive import-all-table from mysql to myhivedatabase as textfile
====================================

sqoop import-all-tables \
...
--warehouse-dir "/user/hive/warehouse/myhivedatabase.db" \
--hive-import \
--hive-database myhivedatabase \
--create-hive-table \
--as-textfile;


========================================================================
### sqoop export: update product_id and insert new records
====================================

--update-key product_id \
--update-mode allowinsert









========================================================================
### Read text file from "/user/cloudera/folder/file.txt" to spark (variable dataFile)
============================

var dataFile = sc.textFile("/user/cloudera/folder/file.txt")

========================================================================
### Read csv file from "/user/cloudera/folder/file.csv" to spark (variable dataFile)
============================

var dataFile = sc.textFile("/user/cloudera/folder/file.csv")


========================================================================
*
### Read/load products from local file system and convert into RDD /data/retail_db/products/part-00000
============================

import scala.io.Source
val productsRaw = Source.
fromFile("/data/retail_db/products/part-00000").
getLines.
toList

val products = sc.parallelize(productsRaw)

========================================================================
### Read avro file from "/user/cloudera/folder" to spark (variable dataFile)
============================
import com.databricks.spark.avro._;
var dataFile = sqlContext.read.avro("/user/cloudera/folder")



========================================================================
*
### Read sequence file from "/user/cloudera/folder" to spark (variable seqData)
### save as orc to /user/cloudera/myorc
============================

var seqData = sequenceFile("/user/cloudera/folder/",classOf[org.apache.hadoop.io.Text],classOf[org.apache.hadoop.io.Text]);

seqData.map(x=>{var d=x._2.toString.split("\t");(d(0),d(1),d(2),d(3))}).toDF().write.orc("/user/cloudera/myorc")



====================================================================================
### PARQUET compression
### use gzip, snappy, uncompressed, lzo compression and save sqlResult rdd 
### as parquet to /user/cloudera/folder
## save as parquet and partition to 1
============================

sqlContext.setConf("spark.sql.parquet.compression.codec","gzip");
sqlContext.setConf("spark.sql.parquet.compression.codec","snappy");
sqlContext.setConf("spark.sql.parquet.compression.codec","uncompressed");
sqlContext.setConf("spark.sql.parquet.compression.codec","lzo");

sqlResult.write.parquet("/user/cloudera/folder")

sqlResult.repartition(1).write.parquet("/user/cloudera/folder")



====================================================================================
### AVRO compression
### use gzip, snappy, uncompressed, lzo compression and save DFResult rdd as avro to /user/cloudera/folder
============================

import com.databricks.spark.avro._;
sqlContext.setConf("spark.sql.avro.compression.codec","gzip");
sqlContext.setConf("spark.sql.avro.compression.codec","snappy");
sqlContext.setConf("spark.sql.avro.compression.codec","uncompressed");
sqlContext.setConf("spark.sql.avro.compression.codec","lzo");

DFResult.write.avro("/user/cloudera/folder");



====================================================================================
### JSON compression
### read avro user/cloudera/myavro, set as dataFile variable
### use gzip, uncompressed compression and save dataFile rdd as JSON to /user/cloudera/folder
============================

import com.databricks.spark.avro._;
var dataFile = sqlContext.read.avro("/user/cloudera/myavro")

dataFile.toJSON.saveAsTextFile("/user/cloudera/folder",classOf[org.apache.hadoop.io.compress.GzipCodec])

sqlContext.setConf("spark.sql.avro.compression.codec","uncompressed")
dataFile.toJSON.saveAsTextFile("/user/cloudera/folder")




====================================================================================
*
### TAB delimited file compression
### you have a the following rdd 
###
### 		dataFile.map(x=>x(0)+"\t"+x(1)+"\t"+x(2)+"\t"+x(3))
###
### use gzip, snappy compression and save the given rdd as text to /user/cloudera/folder
### save as sequence using no compression 
============================

dataFile.map(x=>x(0)+"\t"+x(1)+"\t"+x(2)+"\t"+x(3)).saveAsTextFile("/user/cloudera/folder",classOf[org.apache.hadoop.io.compress.GzipCodec]);

dataFile.map(x=>x(0)+"\t"+x(1)+"\t"+x(2)+"\t"+x(3)).saveAsTextFile("/user/cloudera/folder",classOf[org.apache.hadoop.io.compress.SnappyCodec])

sqlContext.setConf("spark.sql.sequence.compression.codec","uncompressed")

dataFile.map(x=>(x(0).toString,x(0)+"\t"+x(1)+"\t"+x(2)+"\t"+x(3))).saveAsSequenceFile("/user/cloudera/folder");



====================================================================================
*
### CSV file compression
### you have a the following rdd 
###
### 		dataFile.map(x=>x(0)+","+x(1)+","+x(2)+","+x(3))
###
### use gzip compression and save the given rdd as csv to /user/cloudera/folder
### save as sequence using no compression 
============================


dataFile.map(x=>x(0)+","+x(1)+","+x(2)+","+x(3)).saveAsTextFile("/user/cloudera/folder",classOf[org.apache.hadoop.io.compress.GzipCodec]);

sqlContext.setConf("spark.sql.sequence.compression.codec","uncompressed")

dataFile.map(x=>(x(0).toString,x(0)+","+x(1)+","+x(2)+","+x(3))).saveAsSequenceFile("/user/cloudera/folder")


====================================================================================
*
### create result table: order_date varchar(255), order_status varchar(255), total_orders int
### create primary key using order_date and order_status
============================

create table result(
order_date varchar(255), 
order_status varchar(255), 
total_orders int
constraint pk_order_result primary key(order_date,order_status)
);



====================================================================================
### change permission on "/user/cloudera/products" folder such that owner has read,write and ### execute permissions, group has read and write permissions whereas others have just read 
### and execute permissions
============================

hdfs dfs -chmod 765 /user/cloudera/products
//Read is 4, Write is 2 and execute is 1. 
//ReadWrite,Execute = 4 + 2 + 1 = 7
//Read,Write = 4+2 = 6
//Read ,Execute=4+1=5




====================================================================================
*
### create external table based on /user/hive/warehouse/retail_stage.db/orders
### name table as orders_sqoop
### table saved to avro
### use this file for schema /user/hive/schemas/order/orders.avsc
============================

create external table orders_sqoop
STORED AS AVRO
LOCATION '/user/hive/warehouse/retail_stage.db/orders'
TBLPROPERTIES ('avro.schema.url'='/user/hive/schemas/order/orders.avsc')





============================================================================================
### start impala in terminal and use hive metadata
==============================================

impala-shell
Invalidate metadata






============================================================================================
*
set partition mode

create a table named orders_avro in hive stored as avro, the table should have same table definition as 
	(order_id int,
	order_date date)
Additionally, this new table should be partitioned by the order_month i.e -> year-order_month.(example: 2014-01)

Load data into orders_avro table from orders_sqoop table.
==========================================================

set hive.exec.dynamic.partition.mode=nonstrict

create table orders_avro
(order_id int,
order_date date)
partitioned by (order_month string)
stored as avro;

insert overwrite table orders_avro partition (order_month)
select 
order_id,
to_date(from_unixtime(cast(order_date/1000 as int))) as order_date,
substring(from_unixtime(cast(order_date/1000 as int)),1,7) as order_month
from default.orders_sqoop;






============================================================================================
*
### copy xml from hive to spark library to make sure the hive spark works well
### spark: use data available on hive metastore
### store mydb.mytable data in new metastore table, named as mydb.newtable
==============================================

sudo ln -s /etc/hive/conf.dist/hive-site.xml /etc/spark/conf/hive-site.xml

var hc=new org.apache.spark.sql.hive.HiveContext(sc);

hc.sql("create table mydb.newtable as select * from mydb.mytable")



