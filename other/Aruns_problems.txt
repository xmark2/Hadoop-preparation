http://arun-teaches-u-tech.blogspot.com/p/certification-preparation-plan.html



// Problem 1
// ================================
// Using sqoop, import orders table into hdfs to folders /user/cloudera/problem1/orders. File should be loaded as Avro
// File and use snappy compression
// ================================
sqoop eval \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--query "select * from products limit 10"

sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table orders \
--compress \
--compression-codec org.apache.hadoop.io.compress.SnappyCodec \
--target-dir /user/cloudera/problem1/orders \
--as-avrodatafile



// ================================
// Using sqoop, import order_items table into hdfs to folders /user/cloudera/problem1/order-items. Files should be
// loaded as avro file and use snappy compression
// ================================
sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table order_items \
--compress \
--compression-codec org.apache.hadoop.io.compress.SnappyCodec \
--target-dir /user/cloudera/problem1/order-items \
--as-avrodatafile




// ================================
// Using Spark Scala load data at /user/cloudera/problem1/orders and /user/cloudera/problem1/orders-items items as
// dataframes.
// ================================
spark-shell --master yarn \
--conf spark.ui.port=12654 \
--num-executors 1 \
--executor-memory 512M
import com.databricks.spark.avro._;
var ordersDF = sqlContext.read.avro("/user/cloudera/problem1/orders")
var orderItemDF = sqlContext.read.avro("/user/cloudera/problem1/order-items")
ordersDF.show()
ordersDF.take(10).foreach(println)

orderitemsDF.show()
orderitemsDF.take(10).foreach(println)
var joinedOrderDataDF = ordersDF.join(orderItemDF,ordersDF("order_id")===orderItemDF("order_item_order_id"))
joinedOrderDataDF.show()






//==========================================================================
// Expected Intermediate Result:
// Order_Date , Order_status, total_orders, total_amount.
// In plain english, please find total orders and total amount per status per day.
// The result should be sorted by
// order date in descending,
// order status in ascending and
// total amount in descending and
// total orders in ascending.
// Aggregation should be done using below methods. However, sorting can be done using a dataframe or RDD. Perform
// aggregation in each of the following ways
// a). Just by using Data Frames API - here order_date should be YYYY-MM-DD format
// b). Using Spark SQL - here order_date should be YYYY-MM-DD format
// c). By using combineByKey function on RDDS -- No need of formatting order_date or total_amount
// =====================================
a). Just by using Data Frames API

var dataFrameResult = joinedOrderDataDF.
groupBy(to_date(from_unixtime(col("order_date")/1000)).alias("formatted_order_date"), col("order_status")).
agg(round(sum(col("order_item_subtotal")),2).alias("total_amount"),
countDistinct(col("order_id")).alias("total_orders")).
orderBy(col("formatted_order_date").desc,col("order_status"),col("total_amount").desc,col("total_orders"))

dataFrameResult.show()

====================================================================================
b). Using Spark SQL

joinedOrderDataDF.registerTempTable("orders")
var sqlResult = sqlContext.sql("select to_date(from_unixtime(order_date/1000)) as order_date, "+
"order_status, cast(sum(order_item_subtotal) as DECIMAL(10,2)) as total_amount, "+
"count(distinct order_id) as total_orders "+
"from orders "+
"group by order_date, order_status "+
"order by order_date desc, order_status, total_amount desc, total_orders")

sqlResult.show()

sqlContext.setConf("spark.sql.parquet.compression.codec","gzip");
sqlResult.write.parquet("/user/cloudera/problem1/result4b-gzip")

sqlContext.setConf("spark.sql.parquet.compression.codec","snappy");
sqlResult.write.parquet("/user/cloudera/problem1/result4b-snappy")
sqlContext.setConf("spark.sql.parquet.compression.codec","uncompressed");
sqlResult.write.parquet("/user/cloudera/problem1/result4b-uncompressed")

sqlContext.setConf("spark.sql.parquet.compression.codec","lzo");
sqlResult.write.parquet("/user/cloudera/problem1/result4b-lzo")

// ===========
// Store the result as CSV file into hdfs using No compression under folder
// ======================

sqlResult.take(10).foreach(println)
var sqlResultCSV = sqlResult.map(rec=>rec(0)+","+rec(1)+","+rec(2)+","+rec(3))
sqlResultCSV.take(10).foreach(println)
sqlResultCSV.saveAsTextFile("/user/cloudera/problem1/result4b-csv")


// ===========
// create a mysql table named result
// and load data from /user/cloudera/problem1/result4a-csv to mysql table named result
// ============================================


mysql -u retail_dba -p cloudera
use retail_db
create table result(
order_date varchar(255) not null,
order_status varchar(255) not null,
total_orders int,
total_amount numeric,
constraint pk_order_result primary key(order_date,order_status)
);
sqoop export \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--table result \
--export-dir "/user/cloudera/problem1/result4b-csv" \
--columns "order_date,order_status,total_amount,total_orders"






// ===============================================================================
// combineByKey solution
// =====
// Expected Intermediate Result:
// Order_Date , Order_status, total_orders, total_amount.
// In plain english, please find total orders and total amount per status per day.
// The result should be sorted by
// order date in descending,
// order status in ascending and
// total amount in descending and
// total orders in ascending.
// Aggregation should be done using below methods. However, sorting can be done using a dataframe or RDD. Perform
// aggregation in each of the following ways
// ==============================================================================================

var comByKeyResult = joinedOrderDataDF.
map(x=> ((x(1).toString,x(3).toString),(x(8).toString.toFloat,x(0).toString))).
combineByKey((x:(Float, String))=>(x._1,Set(x._2)),(x:(Float,Set[String]),y:(Float,String))=>(x._1 + y._1,x._2+y._2),
(x:(Float,Set[String]),y:(Float,Set[String]))=>(x._1+y._1,x._2++y._2)).
map(x=> (x._1._1,x._1._2,x._2._1,x._2._2.size)).
toDF().
orderBy(col("_1").desc,col("_2"),col("_3").desc,col("_4"));
comByKeyResult.show();
var ordersMap = joinedOrderDataDF.map(rec=>(rec(1))).take(10).foreach(println)
var ordersMap = joinedOrderDataDF.
map(x=>((x(1).toString,x(3).toString),(x(8).toString.toFloat,x(0).toString)))

ordersMap.take(10).foreach(println)
var comByKeyResult = ordersMap.combineByKey((x:(Float,
String))=>(x._1,Set(x._2)),(x:(Float,Set[String]),y:(Float,String))=>(x._1 +
y._1,x._2+y._2),(x:(Float,Set[String]),y:(Float,Set[String]))=>(x._1+y._1,x._2++y._2)).
map(x=> (x._1._1,x._1._2,x._2._1,x._2._2.size)).
toDF().
orderBy(col("_1").desc,col("_2"),col("_3").desc,col("_4"));

comByKeyResult.show()














// Problem 2
sqoop eval \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username retail_dba \
--password cloudera \
--query "select * from products"





// ===========================
// Using sqoop copy data available in mysql products table
// to folder /user/cloudera/products on hdfs as text file.
// columns should be delimited by pipe '|'
// ======================================================
sqoop import \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username retail_dba \
--password cloudera \
--table products \
--fields-terminated-by '|' \
--target-dir /user/cloudera/products \
--as-textfile
hdfs dfs -ls /user/cloudera/products
hdfs dfs -cat /user/cloudera/products/*


// ======================================================
// move all the files from /user/cloudera/products */
// folder to /user/cloudera/problem2/products folder
// ======================================================
hdfs dfs -mkdir /user/cloudera/problem2
hdfs dfs -mv /user/cloudera/products /user/cloudera/problem2/products
hdfs dfs -ls /user/cloudera/problem2/products


// ======================================================
// Change permissions of all the files under /user/cloudera/problem2/products
// such that owner has read,write and execute permissions,
// group has read and write permissions
// whereas others have just read and execute permissions
// ======================================================
hdfs dfs -chmod 765 /user/cloudera/problem2/products
//Read is 4, Write is 2 and execute is 1.
//ReadWrite,Execute = 4 + 2 + 1 = 7
//Read,Write = 4+2 = 6
//Read ,Execute=4+1=5




// ======================================================
// read data in /user/cloudera/problem2/products
// and do the following operations using
// a) dataframes api
// b) spark sql
// c) RDDs aggregateByKey method.
// Your solution should have three sets of steps.
// Sort the resultant dataset by category id
// filter such that your RDD\DF has products whose price is lesser than 100 USD
// on the filtered data set find out the highest value in the product_price column under each category
// on the filtered data set also find out total products under each category
// on the filtered data set also find out the average price of the product under each category
// on the filtered data set also find out the minimum price of the product under each category
// ======================================================
var productsDF = sc.textFile("/user/cloudera/problem2/products").map(rec=>rec.split('|'))
var productsMap = productsDF.
map(rec=>(rec(0).toInt,rec(1).toInt,rec(2).toString,rec(3).toString,rec(4).toFloat)).
toDF("id","category_id","name","description","price")
productsMap.take(10).foreach(println)
productsMap.show()


// DataFrame_Api
// --------------------
import org.apache.spark.sql.functions._
var dataFrameResult = productsDF.
filter("productPrice < 100").
groupBy(col("productCategory")).
agg({
max(
col("productPrice")).alias("max_price"),
countDistinct(col("productID")).alias("tot_products"),
round(avg(col("productPrice")),2).alias("avg_price"),
min(col("productPrice")).alias("min_price")}).
orderBy(col("productCategory"));
dataFrameResult.show();


// ======================================================
// store the result in avro file using snappy compression under these folders respectively
// /user/cloudera/problem2/products/result-df

import com.databricks.spark.avro._;
sqlContext.setConf("spark.sql.avro.compression.codec","snappy")
dataFrameResult.write.avro("/user/cloudera/problem2/products/result-df");


// SparkSQL
// ------------------------------------


productsMap.registerTempTable("products")
var result = sqlContext.sql({“””select category_id,
max(price) as highest_price,
count(distinct id) as total_products,
cast(avg(price) as decimal(10,2)) as avg_price,
min(price) as min_price
from products
where price<100 group by category_id order by category_id”””})



// ======================================================
// store the result in avro file using snappy compression under these folders respectively
// /user/cloudera/problem2/products/result-sql
import com.databricks.spark.avro._;
sqlContext.setConf("spark.sql.avro.compression.codec","snappy")
sqlResult.write.avro("/user/cloudera/problem2/products/result-sql")



// RDD aggregateByKey
// ---------------------------
var rddResult = productsDF.
map(x=>(x(1).toString.toInt,x(4).toString.toDouble)).
aggregateByKey((0.0,0.0,0,9999999999999.0))((x,y)=>
(math.max(x._1,y),x._2+y,x._3+1,math.min(x._4,y)),(x,y)=>
(math.max(x._1,y._1),x._2+y._2,x._3+y._3,math.min(x._4,y._4))).
map(x=> (x._1,x._2._1,(x._2._2/x._2._3),x._2._3,x._2._4)).
sortBy(_._1, false);
rddResult.collect().foreach(println);



// ======================================================
// store the result in avro file using snappy compression under these folders respectively
// /user/cloudera/problem2/products/result-rdd
// ======================================================
import com.databricks.spark.avro._;
sqlContext.setConf("spark.sql.avro.compression.codec","snappy")
rddResult.toDF().write.avro("/user/cloudera/problem2/products/result-rdd");;
hdfs dfs -ls /user/cloudera/problem2/products/result-sql



















// Problem 3
// ====================================
// Import all tables from mysql database into hdfs as avro data files. use compression and the compression codec should
// be snappy. data warehouse directory should be retail_stage.db
// ====================================
sqoop import-all-tables \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username retail_dba \
--password cloudera \
--warehouse-dir /user/hive/warehouse/retail_stage.db \
--as-avrodatafile \
--compress \
--compression-codec org.apache.hadoop.io.compress.SnappyCodec \
-m 1



// ====================================
// Create a metastore table that should point to the orders data imported by sqoop job above. Name the table
// orders_sqoop.
// ====================================
hdfs dfs -ls /user/hive/warehouse/retail_stage.db
mkdir avro-test
cd avro-test/

hadoop fs -get /user/hive/warehouse/retail_stage.db/orders/part-m-00000.avro
avro-tools getschema part-m-00000.avro > orders.avsc
ls
view orders.avsc
vi orders.avsc
hdfs dfs -mkdir /user/hive/schemas
hdfs dfs -ls /user/hive/schemas/order
hdfs dfs -mkdir /user/hive/schemas/order
hdfs dfs -copyFromLocal orders.avsc /user/hive/schemas/order
create external table orders_sqoop
STORED AS AVRO
LOCATION '/user/hive/warehouse/retail_stage.db/orders'
TBLPROPERTIES ('avro.schema.url'='/user/hive/schemas/order/orders.avsc')



// ====================================
// Write query in hive that shows all orders belonging to a certain day. This day is when the most orders were placed.
// select data from orders_sqoop.
// ====================================
// ===step 1
hive> describe orders_sqoop;
OK
order_id int
order_date bigint
order_customer_id int
order_status string
Time taken: 0.072 seconds, Fetched: 4 row(s)
===step 2
select order_date
from (
select order_date, count(1) as orders
from orders_sqoop
group by order_date
order by order_date desc
limit 1
) inner

===step 3
select *
from orders_sqoop x
where x.order_date in
(select order_date
from (
select order_date, count(1) as orders
from orders_sqoop
group by order_date
order by order_date desc
limit 1
) inner)



// ============
// query table in impala that shows all orders belonging to a certain day. This day is when the most orders were placed.
// select data from order_sqoop.
// =========
impala-shell
Invalidate metadata
select *
from orders_sqoop x
where x.order_date in
(select order_date
from (
select order_date, count(1) as orders
from orders_sqoop
group by order_date
order by order_date desc
limit 1
) imp)






// ============
// Now create a table named retail.orders_avro in hive stored as avro, the table should have same table definition as

// order_sqoop. Additionally, this new table should be partitioned by the order month i.e -> year-
// order_month.(example: 2014-01)

// ============
order_id int
order_date bigint
order_customer_id int
order_status string
create database retail;
create table orders_avro
(order_id int,
order_date date,
order_customer_id int,
order_status string)
partitioned by (order_month string)
stored as avro;




// ============
// Load data into orders_avro table from orders_sqoop table.
// ============
set hive.exec.dynamic.partition.mode=nonstrict
insert overwrite table orders_avro partition (order_month)
select
order_id,
to_date(from_unixtime(cast(order_date/1000 as int))) as order_date,
order_customer_id,
order_status,
substring(from_unixtime(cast(order_date/1000 as int)),1,7) as order_month
from default.orders_sqoop;




// ============
// Write query in hive that shows all orders belonging to a certain day. This day is when the most orders were placed.
// select data from orders_avro
// ============

select *
from orders_avro x
where x.order_date in
(select order_date
from (
select order_date, count(1) as orders
from orders_avro
group by order_date
order by order_date desc
limit 1
) avr)


// ==========
// evolve the avro schema related to orders_sqoop table by adding more fields named (order_style String, order_zone
// Integer)
// ==========
hdfs dfs -get /user/hive/schemas/order/orders.avsc
gedit orders.avsc
, {
"name" : "order_style",
"type" : [ "null", "string" ],
"default" : null,
"columnName" : "order_style",
"sqlType" : "12"
}, {
"name" : "order_zone",

"type" : [ "null", "int" ],
"default" : null,
"columnName" : "order_zone",
"sqlType" : "4"
},

hdfs dfs -copyFromLocal -f orders.avsc /user/hive/schemas/order/orders.avsc




// ==========
// insert two more records into orders_sqoop table.
// ==========
order_id int
order_date bigint
order_customer_id int
order_style string
order_zone int
order_status string
insert into orders_sqoop (order_id, order_date, order_customer_id, order_style, order_zone, order_status)
values (68886,1406098800000,5533,"abc",3,"COMPLETE");
insert into orders_sqoop (order_id, order_date, order_customer_id, order_style, order_zone, order_status)
values (68887,1407098800000,5533,"abc",4,"ON_HOLD");




// ==========
// Write query in hive that shows all orders belonging to a certain day. This day is when the most orders were placed.
// select data from orders_sqoop
// ==========

select *
from orders_sqoop x
where x.order_date in
(select order_date
from (
select order_date, count(1) as orders
from orders_sqoop
group by order_date
order by order_date desc
limit 1
) inner)




// ==========
// query table in impala that shows all orders belonging to a certain day. This day is when the most orders were placed.
// select data from orders_sqoop
// ==========
invalidate metadata;
select *
from orders_sqoop x
where x.order_date in
(select order_date
from (
select order_date, count(1) as orders
from orders_sqoop
group by order_date
order by order_date desc
limit 1
) imp);












// Problem 4
// ====================================
// 1.
// Import orders table from mysql as text file to the destination /user/cloudera/problem5/text. Fields should be
// terminated by a tab character ("\t") character and lines should be terminated by new line character ("\n").
// ====================================
sqoop import \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username retail_dba \
--password cloudera \
--table orders \
--target-dir /user/cloudera/problem5/text \
--fields-terminated-by '\t' \
--lines-terminated-by '\n' \
--as-textfile \
-m 1



// ======================================
// 2.
// Import orders table from mysql into hdfs to the destination /user/cloudera/problem5/avro. File should be stored as
// avro file.
// =======================================
sqoop import \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username retail_dba \
--password cloudera \
--table orders \
--target-dir /user/cloudera/problem5/avro \
--as-avrodatafile \
-m 1



// ======================================
// 3.
// Import orders table from mysql into hdfs to folders /user/cloudera/problem5/parquet. File should be stored as
// parquet file.
// ======================================

sqoop import \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username retail_dba \
--password cloudera \
--table orders \
--target-dir /user/cloudera/problem5/parquet \
--as-parquetfile \
-m 1

// ===========================
// 4.
// Transform/Convert data-files at /user/cloudera/problem5/avro and store the converted file at the following locations
// and file formats
// --------------------------

// save the data to hdfs using snappy compression as parquet file at /user/cloudera/problem5/parquet-snappy-
// compress

// --------------------------
// save the data to hdfs using gzip compression as text file at /user/cloudera/problem5/text-gzip-compress
// --------------------------
// save the data to hdfs using no compression as sequence file at /user/cloudera/problem5/sequence
// --------------------------
// save the data to hdfs using snappy compression as text file at /user/cloudera/problem5/text-snappy-compress
// =============================

var dataFile = sqlContext.read.avro("/user/cloudera/problem5/avro");
sqlContext.setConf("spark.sql.parquet.compression.codec","snappy");
dataFile.repartition(1).write.parquet("/user/cloudera/problem5/parquet-snappy-compress")

dataFile.map(x=>x(0)+"\t"+x(1)+"\t"+x(2)+"\t"+x(3)).saveAsTextFile("/user/cloudera/problem5/text-gzip-
compress",classOf[org.apache.hadoop.io.compress.GzipCodec]);

dataFile.map(x=>(x(0).toString,x(0)+"\t"+x(1)+"\t"+x(2)+"\t"+x(3))).saveAsSequenceFile("/user/cloudera/problem5/sequ
ence");

dataFile.map(x=>x(0)+"\t"+x(1)+"\t"+x(2)+"\t"+x(3)).saveAsTextFile("/user/cloudera/problem5/text-snappy-
compress",classOf[org.apache.hadoop.io.compress.SnappyCodec])

// ============================
// 5.
// Transform/Convert data-files at /user/cloudera/problem5/parquet-snappy-compress and store the converted file at
// the following locations and file formats
// --------------------------
// save the data to hdfs using no compression as parquet file at /user/cloudera/problem5/parquet-no-compress
// ------------------------
// save the data to hdfs using snappy compression as avro file at /user/cloudera/problem5/avro-snappy
// ============================

var dataFile = sqlContext.read.parquet("/user/cloudera/problem5/parquet-snappy-compress")
sqlContext.setConf("spark.sql.parquet.compression.codec","uncompressed")
dataFile.write.parquet("/user/cloudera/problem5/parquet-no-compress")
sqlContext.setConf("spark.sql.avro.compression.codec","snappy")
dataFile.write.avro("/user/cloudera/problem5/avro-snappy")

// ============================
// 6.
// Transform/Convert data-files at /user/cloudera/problem5/avro-snappy and store the converted file at the following
// locations and file formats
// ------------------------
// save the data to hdfs using no compression as json file at /user/cloudera/problem5/json-no-compress
// ------------------------
// save the data to hdfs using gzip compression as json file at /user/cloudera/problem5/json-gzip
// ============================
var dataFile = sqlContext.read.avro("/user/cloudera/problem5/avro-snappy")
sqlContext.setConf("spark.sql.avro.compress.codec","uncompressed")
dataFile.toJSON.saveAsTextFile("/user/cloudera/problem5/json-no-compress")

dataFile.toJSON.saveAsTextFile("/user/cloudera/problem5/json-
gzip",classOf[org.apache.hadoop.io.compress.GzipCodec])

// =============================
// 7.
// Transform/Convert data-files at /user/cloudera/problem5/json-gzip and store the converted file at the following
// locations and file formats
// ------------------------
// save the data to as comma separated text using gzip compression at /user/cloudera/problem5/csv-gzip
// ==============================
var dataFile = sqlContext.read.json("/user/cloudera/problem5/json-gzip")
dataFile.map(rec=>rec(0)+","+rec(1)+","+rec(2)+","+rec(3)).take(10).foreach(println)

dataFile.map(rec=>rec(0)+","+rec(1)+","+rec(2)+","+rec(3)).saveAsTextFile("/user/cloudera/problem5/csv-
gzip",classOf[org.apache.hadoop.io.compress.GzipCodec])

// =============================
// 8.
// Using spark access data at /user/cloudera/problem5/sequence and stored it back to hdfs using no compression as
// ORC file to HDFS to destination /user/cloudera/problem5/orc
// =============================
hdfs dfs -get /user/cloudera/problem5/sequence/part-00000
cut -c-300 part-00000
view -c-500 part-00000
org.apache.hadoop.io.Text
var seqData =
sc.sequenceFile("/user/cloudera/problem5/sequence/",classOf[org.apache.hadoop.io.Text],classOf[org.apache.hadoop.i
o.Text]);
seqData.map(x=>{var d=x._2.toString.split("\t");(d(0),d(1),d(2),d(3))}).toDF().collect.foreach(println)
seqData.map(x=>{var d=x._2.toString.split("\t");(d(0),d(1),d(2),d(3))}).toDF().write.orc("/user/cloudera/problem5/orc")


















// Problem 5
// prep
// =============================
mysql -u retail_dba -p cloudera
create table products_replica as select * from products;
alter table products_replica add primary key (product_id);
alter table products_replica add column (product_grade int, product_sentiment varchar(100));
update products_replica set product_grade=1 where product_price>500;
update products_replica set product_sentiment='WEAK' where product_price between 300 and 500;
select product_grade,product_price,product_name from products_replica where product_price>500;
select product_sentiment,product_price,product_name from products_replica where product_price between 300 and
500;




// =============================
// 1.
// Using sqoop, import products_replica table from MYSQL into hdfs such that fields are separated by a '|' and lines are
// separated by '\n'. Null values are represented as -1 for numbers and "NOT-AVAILABLE" for strings. Only records with
// product id greater than or equal to 1 and less than or equal to 1000 should be imported and use 3 mappers for
// importing. The destination file should be stored as a text file to directory /user/cloudera/problem5/products-text.
// =============================
sqoop import \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username retail_dba \
--password cloudera \
--table products_replica \
--fields-terminated-by '|' \
--lines-terminated-by '\n' \
--null-non-string -1 \
--null-string "NOT-AVAILABLE" \
--where "product_id between 1 and 1000" \
--target-dir /user/cloudera/problem5/products-text \
--boundary-query "select min(product_id),max(product_id) from products_replica where product_id between 1 and
1000" \
--outdir /home/cloudera/sqoop1 \
-m 3;




// =============================
// 2.
// Using sqoop, import products_replica table from MYSQL into hdfs such that fields are separated by a '*' and lines are
// separated by '\n'. Null values are represented as -1000 for numbers and "NA" for strings. Only records with product id
// less than or equal to 1111 should be imported and use 2 mappers for importing. The destination file should be stored
// as a text file to directory /user/cloudera/problem5/products-text-part1.
// =============================

sqoop import \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username retail_dba \
--password cloudera \
--table products_replica \
--fields-terminated-by '*' \
--lines-terminated-by '\n' \
--null-non-string -1000 \
--null-string "NA" \
--where "product_id<=1111" \
--as-textfile \
--target-dir /user/cloudera/problem5/products-text-part1 \
--outdir /home/cloudera/sqoop2 \
--boundary-query "select min(product_id),max(product_id) from products_replica where product_id<=1111" \
-m 2;


// =============================
// 3.
// Using sqoop, import products_replica table from MYSQL into hdfs such that fields are separated by a '*' and lines are
// separated by '\n'. Null values are represented as -1000 for numbers and "NA" for strings. Only records with product id
// greater than 1111 should be imported and use 5 mappers for importing. The destination file should be stored as a text
// file to directory /user/cloudera/problem5/products-text-part2.
// =============================

sqoop import \
--connect jdbc:mysql://quickstart.cloudera:3306/retail_db \
--username retail_dba \
--password cloudera \
--table products_replica \
--fields-terminated-by '*' \
--lines-terminated-by '\n' \
--null-non-string -1000 \
--null-string "NA" \
--where "product_id>1111" \
--as-textfile \
--target-dir /user/cloudera/problem5/products-text-part2 \
--outdir /home/cloudera/sqoop3 \
--boundary-query "select min(product_id),max(product_id) from products_replica where product_id>1111" \
-m 5;

// =============================
// 4.
// Using sqoop merge data available in /user/cloudera/problem5/products-text-part1 and

// /user/cloudera/problem5/products-text-part2 to produce a new set of files in /user/cloudera/problem5/products-
// text-both-parts

// =============================



sqoop merge --help > sm.txt
#run task 1/2 and get jar file
/tmp/sqoop-cloudera/compile/49c324963181b55c4331c4b761070f72/products_replica.jar
sqoop help
------------------------------------------
#list all merge arguments
sqoop merge help
#save help screen to a file
sqoop merge --help > sm.txt
---------------------
ls /home/cloudera/sqoop3
------------------------------------------
sqoop merge \
--class-name products_replica \
--jar-file /tmp/sqoop-cloudera/compile/49c324963181b55c4331c4b761070f72/products_replica.jar \
--new-data /user/cloudera/problem5/products-text-part2 \
--onto /user/cloudera/problem5/products-text-part1 \
--target-dir /user/cloudera/problem5/products-text-both-parts \
--merge-key product_id;

#checking
mkdir sqoop4
cd sqoop4
hdfs dfs -get /user/cloudera/problem5/products-text-both-parts
ls
gedit part-r-00000

// ==================================
// 5.
// Using sqoop do the following. Read the entire steps before you create the sqoop job.
// ------------------

// create a sqoop job Import Products_replica table as text file to directory /user/cloudera/problem5/products-
// incremental. Import all the records.

// ------------------
// insert three more records to Products_replica from mysql
// ------------------
// run the sqoop job again so that only newly added records can be pulled from mysql
// ------------------
// insert 2 more records to Products_replica from mysql
// ------------------
// run the sqoop job again so that only newly added records can be pulled from mysql
// ------------------
// Validate to make sure the records have not be duplicated in HDFS
// =============================
sqoop job --create first_sqoop_job \
-- import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username "retail_dba" \
--password "cloudera" \
--table products_replica \
--target-dir /user/cloudera/problem5/products-incremental \
--check-column product_id \
--incremental append \
--last-value 0;

mysql -u retail_dba -p cloudera
insert into products_replica values (1346,2,'something 1346','something 2',300.00,'not avaialble',3,'STRONG');
insert into products_replica values (1347,5,'something 1347','something 2',356.00,'not avaialble',3,'STRONG');
insert into products_replica values (1348,4,'something 1348','something 2',1.00,'not avaialble',3,'WEAK');
sqoop job --exec first_sqoop_job;

insert into products_replica values (1376,4,'something 1376','something 2',1.00,'not avaialble',3,'WEAK');
insert into products_replica values (1377,4,'something 1377','something 2',10.00,'not avaialble',null,'NOT APPLICABLE');
sqoop job --exec first_sqoop_job;
sqoop job --delete first_sqoop_job;

// ==================================
// 6.
// Using sqoop do the following. Read the entire steps before you create the sqoop job.
// ------------------
// create a hive table in database named problem5 using below command
// ------------------
// create table products_hive (product_id int, product_category_id int, product_name string, product_description
// string, product_price float, product_imaage string,product_grade int, product_sentiment string);
// ------------------
// create a sqoop job Import Products_replica table as hive table to database named problem5. name the table as
// products_hive.
// ------------------
// insert three more records to Products_replica from mysql
// ------------------
// run the sqoop job again so that only newly added records can be pulled from mysql
// ------------------
// insert 2 more records to Products_replica from mysql
// ------------------
// run the sqoop job again so that only newly added records can be pulled from mysql
// ------------------
// Validate to make sure the records have not been duplicated in Hive table
// =============================

create database problem5;
create table products_hive (product_id int, product_category_id int, product_name string, product_description string,
product_price float, product_image string,product_grade int, product_sentiment string);
sqoop job --create hive_sqoop_job \
-- import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username "retail_dba" \
--password "cloudera" \
--table products_replica \
--check-column product_id \
--incremental append \l
--last-value 0 \
--hive-import \
--hive-database problem5 \
--hive-table products_hive;
sqoop job --delete hive_sqoop_job;
sqoop job --list;
sqoop job --exec hive_sqoop_job;
hive
use problem5
select * from products_hive order by product_id desc;

mysql -u retail_dba -p cloudera
insert into products_replica values (1346,2,'something 1346','something 2',300.00,'not avaialble',3,'STRONG');
insert into products_replica values (1347,5,'something 1347','something 2',356.00,'not avaialble',3,'STRONG');
insert into products_replica values (1348,4,'something 1348','something 2',1.00,'not avaialble',3,'WEAK');

hive
use problem5
select * from products_hive order by product_id desc;
sqoop job --exec hive_sqoop_job;
insert into products_replica values (1376,4,'something 1376','something 2',1.00,'not avaialble',3,'WEAK');
insert into products_replica values (1377,4,'something 1377','something 2',10.00,'not avaialble',null,'NOT APPLICABLE');

sqoop job --exec hive_sqoop_job;
hive
use problem5
select * from products_hive order by product_id desc;



// ==================================
// 7.
// Using sqoop do the following. .
// ------------------
// insert 2 more records into products_hive table using hive.
// ------------------
// create table in mysql using below command
// ------------------
// create table products_external (product_id int(11) primary Key, product_grade int(11), product_category_id
// int(11), product_name varchar(100), product_description varchar(100), product_price float, product_image
// varchar(500), product_sentiment varchar(100));
// ------------------
// export data from products_hive (hive) table to (mysql) products_external table.
// ------------------
// insert 2 more records to Products_hive table from hive
// ------------------
// export data from products_hive table to products_external table.
// ------------------
// Validate to make sure the records have not be duplicated in mysql table
// =============================

hive
use problem5
insert into table products_hive values (1380,4,'something 1380','something 2',8.00,'not avaialble',3,'NOT APPLICABLE');
insert into table products_hive values (1381,4,'something 1381','something 2',8.00,'not avaialble',3,'NOT APPLICABLE');
select count(1) from products_hive;
mysql -u retail_dba -p cloudera
select count(1) from products_external;
create table products_external (product_id int(11) primary Key, product_grade int(11), product_category_id int(11),
product_name varchar(100), product_description varchar(100), product_price float, product_image varchar(500),
product_sentiment varchar(100));
sqoop export \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username "retail_dba" \
--password "cloudera" \
--table products_external \
--export-dir /user/hive/warehouse/problem5.db/products_hive/ \
--input-fields-terminated-by "\001" \
--input-null-non-string "null" \
--input-null-string "null" \
--update-key product_id \
--update-mode allowinsert \
--columns
"product_id,product_category_id,product_name,product_description,product_price,product_image,product_grade,pro
duct_sentiment";

hive

use problem5
insert into table products_hive values (1382,4,'something 1382','something 2',8.00,'not avaialble',3,'NOT APPLICABLE');
insert into table products_hive values (1383,4,'something 1383','something 2',8.00,'not avaialble',3,'NOT APPLICABLE');
select count(1) from products_hive;
sqoop export \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username "retail_dba" \
--password "cloudera" \
--table products_external \
--export-dir /user/hive/warehouse/problem5.db/products_hive/ \
--input-fields-terminated-by "\001" \
--input-null-non-string "null" \
--input-null-string "null" \
--update-key product_id \
--update-mode allowinsert \
--columns
"product_id,product_category_id,product_name,product_description,product_price,product_image,product_grade,pro
duct_sentiment";
mysql -u retail_dba -p cloudera
select count(1) from products_external;
insert into table products_hive values (1384,4,'something 1384','something 2',8.00,'not avaialble',null,'NOT
APPLICABLE');
insert into table products_hive values (1385,4,'something 1385','something 2',8.00,'not avaialble',3,'NOT APPLICABLE');












// Problem 6

// ============================
// 1.
// create a hive meta store database named problem6 and import all tables from mysql retail_db database into hive
// meta store.
// =============================
create database problem6;
/user/hive/warehouse/problem6.db/test
sqoop import-all-tables \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username "retail_dba" \
--password "cloudera" \
--warehouse-dir "/user/hive/warehouse/problem6.db" \
--hive-import \
--hive-database problem6 \
--create-hive-table \
--as-textfile;
sqoop import-all-tables \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username retail_dba \
--password cloudera \
--warehouse-dir /user/hive/warehouse/problem6.db \
--hive-import \
--hive-database problem6 \
--create-hive-table \
--as-textfile;

// ============================
// 2.
// On spark shell use data available on meta store as source and perform step 3,4,5 and 6.
// =============================

spark-shell --master yarn \
--conf spark.ui.port=12654 \
--num-executors 2 \
--executor-memory 512M
var hc=new org.apache.spark.sql.hive.HiveContext(sc);
sudo ln -s /etc/hive/conf.dist/hive-site.xml /etc/spark/conf/hive-site.xml

// ============================
// 3.
// Rank products within department by price and order by department ascending and rank descending
// ============================
var hc = new org.apache.spark.sql.hive.HiveContext(sc);
var result = hc.sql("show tables").show()
var result = hc.sql({"""select
category_id
from problem6.categories"""});
var hiveResult = hc.sql({"""
select
rank() over (partition by d.department_id order by p.product_price) as rank,
dense_rank() over (partition by d.department_id order by p.product_price) as dense_rank,
d.department_id,
p.product_name,
p.product_price,
p.product_id
from problem6.products p
inner join problem6.categories c
on p.product_category_id = c.category_id
inner join problem6.departments d
on c.category_department_id=d.department_id
order by d.department_id, dense_rank, rank desc"""});

// ==================================
// 4.
// find top 10 customers with most unique product purchases. if more than one customer has the same number of
// product purchases then the customer with the lowest customer_id will take precedence
// ==================================
var hiveResult2 = hc.sql({"""
select
customer_id, customer_fname,unique_product_purchases
from
(select c.customer_id
,c.customer_fname
,count(distinct(oi.order_item_product_id)) as unique_product_purchases
,row_number() over (partition by count(distinct(oi.order_item_product_id)) order by c.customer_id) as filter
from problem6.customers c
inner join problem6.orders o
on c.customer_id=o.order_customer_id
inner join problem6.order_items oi
on o.order_id=oi.order_item_order_id
group by c.customer_id,c.customer_fname
) main
where filter=1
order by unique_product_purchases desc, customer_id
limit 10"""});

scala> hiveResult2.show();
+-----------+--------------+------------------------+
|customer_id|customer_fname|unique_product_purchases|
+-----------+--------------+------------------------+
| 1288| Evelyn| 17|
| 2292| Ashley| 16|
| 329| Samantha| 15|
| 791| Mary| 14|
| 173| Jose| 13|
| 39| Juan| 12|
| 9| Mary| 11|
| 12| Christopher| 10|
| 3| Ann| 9|
| 6| Mary| 8|
+-----------+--------------+------------------------+

// ==================================
// 5.
// On dataset from step 3, apply filter such that only products less than 100 are extracted
// ==================================
hiveResult.registerTempTable("product_rank_result_temp");
hc.sql("select * from product_rank_result_temp where product_price<100");

// ==================================
// 6.
// On dataset from step 4, extract details of products purchased by top 10 customers which are priced at less than 100
// USD per unit
// ==================================
hiveResult2.registerTempTable("top_Customers");
hc.sql("select customer_id from top_Customers").show();
+-----------+
|customer_id|
+-----------+
| 1288|
| 2292|
| 329|
| 791|
| 173|
| 39|
| 9|
| 12|
| 3|
| 6|
+-----------+

hc.sql({"""
select t.customer_id, p.*
from problem6.orders o
inner join problem6.order_items oi
on o.order_id=oi.order_item_order_id
inner join problem6.products p
on oi.order_item_product_id=p.product_id
inner join top_Customers t
on o.order_customer_id=t.customer_id
where p.product_price<100"""});

hc.sql({"""
select distinct p.*
from problem6.orders o
inner join problem6.order_items oi
on o.order_id=oi.order_item_order_id
inner join problem6.products p
on oi.order_item_product_id=p.product_id
inner join top_Customers t
on o.order_customer_id=t.customer_id
where p.product_price<100"""});

hive> describe order_items;
order_item_id int
order_item_order_id int
order_item_product_id int
order_item_quantity tinyint
order_item_subtotal double
order_item_product_price double
hive> describe orders;
order_id int
order_date string
order_customer_id int
order_status string
hive> describe products;
product_id int
product_category_id int
product_name string
product_description string
product_price double
product_image string

// ==================================
// 7.
// Store the result of 5 and 6 in new meta store tables within hive.
// ==================================

hc.sql("create table problem6.top_product_result as select * from product_rank_result_temp where
product_price<100");
hc.sql({"""
create table problem6.top_products as
select distinct p.*
from problem6.orders o
inner join problem6.order_items oi
on o.order_id=oi.order_item_order_id
inner join problem6.products p
on oi.order_item_product_id=p.product_id
inner join top_Customers t
on o.order_customer_id=t.customer_id
where p.product_price<100"""});















// Problem 7
// ==================================
// 1.
// This step comprises of three substeps. Please perform tasks under each subset completely
// --------------------------
// 1.1
// using sqoop pull data from MYSQL orders table into /user/cloudera/problem7/prework as AVRO data file using
// only one mapper
// --------------------------
// 1.2
// Pull the file from \user\cloudera\problem7\prework into a local folder named flume-avro
// --------------------------
// 1.3
// create a flume agent configuration such that it has an avro source at localhost and port number 11112, a jdbc
// channel and an hdfs file sink at /user/cloudera/problem7/sink
// --------------------------
// 1.4
// Use the following command to run an avro client flume-ng avro-client -H localhost -p 11112 -F <<Provide your avro
// file path here>>
// ===================================
1.1
--------------------------
sqoop import \
--connect "jdbc:mysql://quickstart.cloudera:3306/retail_db" \
--username "retail_dba" \
--password "cloudera" \
--table orders \
--target-dir /user/cloudera/problem7/prework \
--as-avrodatafile \
-m 1
1.2
--------------------------
mkdir flume-avro
cd flume-avro
hdfs dfs -get /user/cloudera/problem7/prework/*

1.3
--------------------------
gedit f.config
--------------------------
#Agent Name = step1
# Name the source, channel and sink
step1.sources = avro-source
step1.channels = jdbc-channel
step1.sinks = file-sink
# Source configuration
step1.sources.avro-source.type = avro
step1.sources.avro-source.port = 11112
step1.sources.avro-source.bind = localhost

# Describe the sink
step1.sinks.file-sink.type = hdfs
step1.sinks.file-sink.hdfs.path = /user/cloudera/problem7/sink
step1.sinks.file-sink.hdfs.fileType = DataStream
step1.sinks.file-sink.hdfs.fileSuffix = .avro
step1.sinks.file-sink.serializer = avro_event
step1.sinks.file-sink.serializer.compressionCodec=snappy
# Describe the type of channel -- Use memory channel if jdbc channel does not work
step1.channels.jdbc-channel.type = jdbc
# Bind the source and sink to the channel
step1.sources.avro-source.channels = jdbc-channel
step1.sinks.file-sink.channel = jdbc-channel

1.4
--------------------------
##Run the flume agent in terminal_1
-----------------------------
flume-ng agent --name step1 --conf . --conf-file f.config

##Run the flume Avro client in terminal_2
---------------------------
cd flume-avro
flume-ng avro-client -H localhost -p 11112 -F part-m-00000.avro

##check results in terminal_3
---------------------------
hdfs dfs -ls /user/cloudera/problem7/sink | wc -l

// ==================================
// 2.
// The CDH comes prepackaged with a log generating job. start_logs, stop_logs and tail_logs. Using these as an aid and
// provide a solution to below problem. The generated logs can be found at path /opt/gen_logs/logs/access.log
// --------------------------
// run start_logs
// --------------------------
// write a flume configuration such that the logs generated by start_logs are dumped into HDFS at location
// /user/cloudera/problem7/step2. The channel should be non-durable and hence fastest in nature. The channel should
// be able to hold a maximum of 1000 messages and should commit after every 200 messages.
// --------------------------
// Run the agent.
// --------------------------
// confirm if logs are getting dumped to hdfs.
// --------------------------
// run stop_logs.
// ==================================
mkdir flume-logs
cd flume-logs
hdfs dfs -mkdir /user/cloudera/problem7/step2
gedit f.config
----------------------------------------------------
# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1
# Describe/configure the source
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /opt/gen_logs/logs/access.log
# Describe the sink
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /user/cloudera/problem7/step2
a1.sinks.k1.hdfs.fileSuffix = .log
a1.sinks.k1.hdfs.writeFormat = Text
a1.sinks.k1.hdfs.fileType = DataStream
# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 200
# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
----------------------------------------------------

hadoop fs -mkdir /user/cloudera/problem7/sink
flume-ng agent --name a1 --conf . --conf-file f.config
hdfs dfs -ls /user/cloudera/problem7/step2 | wc -l